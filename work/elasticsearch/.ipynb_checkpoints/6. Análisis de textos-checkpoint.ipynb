{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de textos\n",
    "\n",
    "Dentro de Elasticsearch nos referimos al análisis de textos al proceso de transformar un texto si estructura como puede ser el cuerpo de un email o la descripción de un producto, en un formato estructurado optimizado para su búsqeuda.\n",
    "\n",
    "Este proceso se puede realizar en dos momentos concretos, al insertar un documento en un índice de Elasticsearch o al realizar una consulta sobre un documento y siempre sobre los campos de tipo text.\n",
    "\n",
    "El análisis de textos es lo que permite a Elasticsearch realizar búsquedas de tipo full-text donde los resultados de las búsquedas devuelven todos los resultados relevantes en vez de sólo los resultados exactos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomía de un analizador\n",
    "Un analizador consta de tres componentes:\n",
    "* **Character Filter**: Recibe el texto original como un stream de caracteres y puede transformar este stream añadiendo, eliminando o modificando estos caracteres. Por ejemplo, puede eliminar las etiquetas de marcado de un texto HTMl. Un analizador puede tener cero o mas Character Filters.\n",
    "* Tokenizer: Recibe un stream de caracteres y lo divide en tokens individuales, habitualemente palabras y como salida devuelve un stream de tokens.Por ejemplo el whitespace tokenizer genera un nuevo token cada vez que encuentra un espacio en balnco. Se encarga además de registrar la posición de inicio y de fin de cada token en el stream de caracteres. Un analizador tiene exactamente un tokenizer.\n",
    "* Token Filter: Recibe el stream de tokens y añade, elimina o modifica los tokens. Por ejemplo un lowercase token filter transforma todos los caracteres en mayúscula de los tokens en minúscula. El stop token filter elimina los tokens que representan las stop words de un lenguaje. Un analizador puede tener cero o mas Token Filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Analyzer\n",
    "\n",
    "Por defecto, sino indicamos lo contrario, Eleasticsearch utiliza el analizador standard para analizar e indexar los campos de tipo text. Vamos a ver como funciona este analizador.\n",
    "\n",
    "El analizador standard consta de los siguientes elementos:\n",
    "* Standard tokenizer\n",
    "* Token filters - Lowercase token filter y opcionalmente Stop Token Filter.\n",
    "\n",
    "Para ver como funciona vamos a utilizar el método _analyze que nos permite probar un analizador sobre un texto y ver su salida:\n",
    "\n",
    "`\n",
    "POST _analyze\n",
    "{\n",
    "  \"analyzer\": \"standard\",\n",
    "  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"tokens\" : [\r\n",
      "    {\r\n",
      "      \"token\" : \"the\",\r\n",
      "      \"start_offset\" : 0,\r\n",
      "      \"end_offset\" : 3,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 0\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"2\",\r\n",
      "      \"start_offset\" : 4,\r\n",
      "      \"end_offset\" : 5,\r\n",
      "      \"type\" : \"<NUM>\",\r\n",
      "      \"position\" : 1\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"quick\",\r\n",
      "      \"start_offset\" : 6,\r\n",
      "      \"end_offset\" : 11,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 2\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"brown\",\r\n",
      "      \"start_offset\" : 12,\r\n",
      "      \"end_offset\" : 17,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 3\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"foxes\",\r\n",
      "      \"start_offset\" : 18,\r\n",
      "      \"end_offset\" : 23,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 4\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"jumped\",\r\n",
      "      \"start_offset\" : 24,\r\n",
      "      \"end_offset\" : 30,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 5\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"over\",\r\n",
      "      \"start_offset\" : 31,\r\n",
      "      \"end_offset\" : 35,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 6\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"the\",\r\n",
      "      \"start_offset\" : 36,\r\n",
      "      \"end_offset\" : 39,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 7\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"lazy\",\r\n",
      "      \"start_offset\" : 40,\r\n",
      "      \"end_offset\" : 44,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 8\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"dogs\",\r\n",
      "      \"start_offset\" : 45,\r\n",
      "      \"end_offset\" : 49,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 9\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"token\" : \"bone\",\r\n",
      "      \"start_offset\" : 50,\r\n",
      "      \"end_offset\" : 54,\r\n",
      "      \"type\" : \"<ALPHANUM>\",\r\n",
      "      \"position\" : 10\r\n",
      "    }\r\n",
      "  ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! curl -X POST \"http://elasticsearch:9200/_analyze?pretty\" -H 'Content-Type: application/json' -d' \\\n",
    "{ \\\n",
    "    \"analyzer\": \"standard\", \\\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\" \\\n",
    "}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch nos permite configurar este analizador para adecuarlo a nuestras necesidades, por ejemplo adaptarlo a nuestro idioma. Para ello podemos utilizar 3 parámetros de configuración.\n",
    "* max_token_length: El tamaño máximo de letras de un token, por defecto 255.\n",
    "* stopwords: Una lista predefinida de stop words, por ejemplo _english_, por defecto _none_\n",
    "* stopwords_path: uri al fichero que contiene la lista de stop words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo:\n",
    "\n",
    "`\n",
    "PUT analyzer-example\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_english_analyzer\": {\n",
    "          \"type\": \"standard\",\n",
    "          \"max_token_length\": 5,\n",
    "          \"stopwords\": \"_english_\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "`\n",
    "\n",
    "`\n",
    "POST analyzer-example/_analyze\n",
    "{\n",
    "  \"analyzer\": \"my_english_analyzer\",\n",
    "  \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n",
    "}\n",
    "`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"acknowledged\" : true,\r\n",
      "  \"shards_acknowledged\" : true,\r\n",
      "  \"index\" : \"analyzer-example2\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! curl -X PUT \"http://elasticsearch:9200/analyzer-example?pretty\" -H 'Content-Type: application/json' -d' \\\n",
    "{ \\\n",
    "  \"settings\": { \\\n",
    "    \"analysis\": { \\\n",
    "      \"analyzer\": { \\\n",
    "        \"my_english_analyzer\": { \\\n",
    "          \"type\": \"standard\", \\\n",
    "          \"max_token_length\": 5, \\\n",
    "          \"stopwords\": \"_english_\" \\\n",
    "        } \\\n",
    "      } \\\n",
    "    } \\\n",
    "  }\\\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST \"http://elasticsearch:9200/analyzer-example/_analyze?pretty\" -H 'Content-Type: application/json' -d' \\\n",
    "{ \\\n",
    "    \"analyzer\": \"my_english_analyzer\", \\\n",
    "    \"text\": \"The 2 QUICK Brown-Foxes jumped over the lazy dogs bone.\" \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steamers\n",
    "\n",
    "Steaming es el proceso de reducir una palabra a su raíz léxica, lo que nos permite utilizar diferentes variantes para buscar las coincidencias sobre las palabras de una búsqueda.\n",
    "Por ejemplo walking y walked se reducen a su raíz walk y las ambas darán lugar a la misma ocurrencia en la búsqueda.\n",
    "\n",
    "Este proceso es dependiente del lenguaje, por lo que necesitaremos utilizar el steamer filter correspondiente al idioma de los documentos que estamos analizando. Un steamer filter es un token filter.\n",
    "\n",
    "Para utilizar correctamente los steamers, vamos a ver como crear nuestro propio analizador:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "PUT /spanish_example\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"spanish_stop\": {\n",
    "          \"type\":       \"stop\",\n",
    "          \"stopwords\":  \"_spanish_\" \n",
    "        },\n",
    "        \"spanish_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"light_spanish\"\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"rebuilt_spanish\": {\n",
    "          \"tokenizer\":  \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"spanish_stop\",\n",
    "            \"spanish_stemmer\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "POST spanish_example/_analyze\n",
    "{\n",
    "  \"analyzer\": \"rebuilt_spanish\",\n",
    "  \"text\": \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor\"\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo usar un analizador\n",
    "\n",
    "Un analizador definido en un mapping type se puede utilizar al indexar un documento para analizar uno o varios campos o bien a la hora de realizar la consulta, para analizar el texto que queremos consultar.\n",
    "\n",
    "`\n",
    "PUT /spanish_example_2\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"spanish_stop\": {\n",
    "          \"type\":       \"stop\",\n",
    "          \"stopwords\":  \"_spanish_\" \n",
    "        },\n",
    "        \"spanish_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"light_spanish\"\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"rebuilt_spanish\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\":  \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"spanish_stop\",\n",
    "            \"spanish_stemmer\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },  \n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"title\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"rebuilt_spanish\",\n",
    "        \"search_analyzer\": \"rebuilt_spanish\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
