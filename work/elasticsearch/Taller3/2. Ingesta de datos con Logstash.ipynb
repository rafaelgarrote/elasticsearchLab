{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Ingesta de datos con Logstash\n",
    "\n",
    "En esta práctica vamos a parender a configurar el servicio de Logstash para ingestar datos en eleastic serch:\n",
    "* Configurar Logstash para leer los ficheros de una carpeta \n",
    "* Parsear un fichero de logs con datos semiestructurados.\n",
    "* Configurar Logstash para escribir los datos procesados en Elasticsarch.\n",
    "\n",
    "La practica consiste en leer los ficheros de log que genera un supuesto servidor web, procesarlos de forma automática, parsear la información y enriquecerla y por último insertarla en Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de nada vamos a ver que formato tienen los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log_lines = open(\"../data/elasticsearch/web_logs/web.log\", \"r\")\n",
    "\n",
    "for line in log_lines:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver el fichero no tiene estructura, pero podemos asumir que hay cierta información que se repite en cada línea del log:\n",
    "\n",
    "* IP que hace la petición\n",
    "* Fehca en la que se realiza la petición\n",
    "* Verbo o método HTTP de la petición\n",
    "* La URL del recurso pedido\n",
    "* El código de la respuesta\n",
    "* El User agent del cliente que hizo la petición\n",
    "* Etc.\n",
    "\n",
    "Por lo que aunque el fichero no tenga estructura, las lineas del log mantienen un formato que vamos a poder parsear y convertir a un formato estructurado a través de Logstash y de los pipelines de ingesta de Elasticsearch.\n",
    "\n",
    "Una vez parseada la linea y estructurada en un documento, vamos a insertar estos documentos en Elasticsearch para poderlos consultar y extraer el conocimiento que necesitemos sobre este servicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Creamos un template para los índices que generemos al insertar los documentos\n",
    "\n",
    "Cuando el volumen de datos que almacenamos es muy grande, es muy común tener varios índies con los documentos que guardamos, de tal forma que podamos aplicar distintas polítcas de gestión de índices en función de la temperatura del dato. Por tanto vamos a crear un índice para cada día de logs.\n",
    "Cada índice que creemos tendrá el siguiente nombre web-logs-{fecha}.\n",
    "\n",
    "Para crear cada índice de forma dinámica y asignarle el mapping type adecuado sin tener que hacerlo de forma manual, vamos a crear un template para todos los índices cuyo nombre cumplan en patrón web-logs-*\n",
    "\n",
    "El mapping que vamos a crear está basado en el Mapping Common Schema (ECS por sus siglas en inglés). ECS es un mapping open source creado por Elastic a partir de las necesidades de la comunidad para almacenar datos provenientes de logs y métricas de diferentes servicios y aplicaciones con el objetivo de tener un esquema estandard para poder sonstrurir sobre él cualquier casos de uso.\n",
    "\n",
    "Entre los beneficios de ECS encontramos:\n",
    "* Simplicidad para relacionar datos de distintas fuentes.\n",
    "* Facilidad de recordar el nombre de campos comunes aun gestionando multitud de fuentes, ya que se mantienen en todas ellas igual.\n",
    "* Posibilidad de reutilizar contenido de análisis previos, tanto visualizaciones y dashboards, como el resto de contenido, ya sean búsquedas, alarmas, reportes o trabajos de Machine Learning existentes.\n",
    "\n",
    "Para más informcación sobre ECS puedes consultar su página de documentación: https://www.elastic.co/guide/en/ecs/current/index.html\n",
    "\n",
    "Para este ejemplo y con el objetivo de poder manejar de forma mas sencilla el mapping desde un notebook nos vamos a quedar únicamente con los campos necesarios para almacenar infromación de un access log de Apache.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "PUT /_index_template/web-logs\n",
    "{\n",
    "  \"index_patterns\": [\"web-logs\"],\n",
    "  \"template\": {\n",
    "    \"mappings\": {\n",
    "      \"dynamic_templates\": [\n",
    "        {\n",
    "          \"strings_as_keyword\": {\n",
    "            \"mapping\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"match_mapping_type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"date_detection\": false,\n",
    "      \"properties\": {        \n",
    "        \"log\": {\n",
    "          \"properties\": {\n",
    "            \"file\": {\n",
    "              \"properties\": {\n",
    "                \"path\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"source\": {\n",
    "          \"properties\": {\n",
    "            \"address\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"ip\": {\n",
    "              \"type\": \"ip\"\n",
    "            },\n",
    "            \"geo\": {\n",
    "              \"properties\": {\n",
    "                \"region_iso_code\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"continent_name\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"city_name\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"country_iso_code\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"name\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"country_name\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"region_name\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"location\": {\n",
    "                  \"type\": \"geo_point\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"as\": {\n",
    "              \"properties\": {\n",
    "                \"number\": {\n",
    "                  \"type\": \"long\"\n",
    "                },\n",
    "                \"organization\": {\n",
    "                  \"properties\": {\n",
    "                    \"name\": {\n",
    "                      \"ignore_above\": 1024,\n",
    "                      \"fields\": {\n",
    "                        \"text\": {\n",
    "                          \"norms\": false,\n",
    "                          \"type\": \"text\"\n",
    "                        }\n",
    "                      },\n",
    "                      \"type\": \"keyword\"\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"url\": {\n",
    "          \"properties\": {\n",
    "            \"original\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\",\n",
    "              \"fields\": {\n",
    "                \"text\": {\n",
    "                  \"norms\": false,\n",
    "                  \"type\": \"text\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"apache\": {\n",
    "          \"properties\": {\n",
    "            \"access\": {\n",
    "              \"properties\": {\n",
    "                \"ssl\": {\n",
    "                  \"properties\": {\n",
    "                    \"cipher\": {\n",
    "                      \"ignore_above\": 1024,\n",
    "                      \"type\": \"keyword\"\n",
    "                    },\n",
    "                    \"protocol\": {\n",
    "                      \"ignore_above\": 1024,\n",
    "                      \"type\": \"keyword\"\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"error\": {\n",
    "              \"properties\": {\n",
    "                \"module\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"@timestamp\": {\n",
    "          \"type\": \"date\"\n",
    "        },\n",
    "        \"http\": {\n",
    "          \"properties\": {\n",
    "            \"request\": {\n",
    "              \"properties\": {\n",
    "                \"referrer\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"method\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"response\": {\n",
    "              \"properties\": {\n",
    "                \"status_code\": {\n",
    "                  \"type\": \"long\"\n",
    "                },\n",
    "                \"body\": {\n",
    "                  \"properties\": {\n",
    "                    \"bytes\": {\n",
    "                      \"type\": \"long\"\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"version\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\"\n",
    "            }\n",
    "          }\n",
    "        },   \n",
    "        \"event\": {\n",
    "          \"properties\": {\n",
    "            \"ingested\": {\n",
    "              \"type\": \"date\"\n",
    "            },\n",
    "            \"outcome\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"original\": {\n",
    "              \"type\": \"text\"\n",
    "            },\n",
    "            \"created\": {\n",
    "              \"type\": \"date\"\n",
    "            },\n",
    "            \"kind\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"category\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"user\": {\n",
    "          \"properties\": {\n",
    "            \"name\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\",\n",
    "              \"fields\": {\n",
    "                \"text\": {\n",
    "                  \"norms\": false,\n",
    "                  \"type\": \"text\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"user_agent\": {\n",
    "          \"properties\": {\n",
    "            \"original\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\",\n",
    "              \"fields\": {\n",
    "                \"text\": {\n",
    "                  \"norms\": false,\n",
    "                  \"type\": \"text\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"os\": {\n",
    "              \"properties\": {\n",
    "                \"name\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"fields\": {\n",
    "                    \"text\": {\n",
    "                      \"norms\": false,\n",
    "                      \"type\": \"text\"\n",
    "                    }\n",
    "                  }\n",
    "                },\n",
    "                \"version\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                },\n",
    "                \"full\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"fields\": {\n",
    "                    \"text\": {\n",
    "                      \"norms\": false,\n",
    "                      \"type\": \"text\"\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"name\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"version\": {\n",
    "              \"ignore_above\": 1024,\n",
    "              \"type\": \"keyword\"\n",
    "            },\n",
    "            \"device\": {\n",
    "              \"properties\": {\n",
    "                \"name\": {\n",
    "                  \"ignore_above\": 1024,\n",
    "                  \"type\": \"keyword\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Para comprobar que se ha creado correctamente entra en la sección Index Templates de Kibana y busca el template que vamos a crear \"web_logs\"\n",
    "\n",
    "\n",
    "* Kibana > menú > Management > Stack Management > Data > Index Management > pestaña Index Templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Creamos un pipeline para procesar los datos\n",
    "\n",
    "Para procesar los datos que lleguen de Logstash, parsearlos, estructurarlos y enriquecerlos vamos a crear un pipeline de ingesta como vimos en la práctica anterior.\n",
    "\n",
    "Todos los processors se entienden con los ejemplos que vimos anteriormente, pero vamos a centrarnos en el processor grok que es más complicado de entender y es realmente donde hacemos toda la \"magia\" de parseo y enriquecimeinto de los datos.\n",
    "\n",
    "El processor grok recibe este nombre del lenguaje que vamos a utilizar para parsear las cadenas de texto de los logs. Este lenguaje se basa en aplicar expresiones regulares sobre cadenas de texto para buscar patrones que se repiten en los datos y que hacen referencia a cierta información. Por ejemplo la expresión %{IP:client} es capáz de aplicar la expresión regular que representa a una IP y asignarselo al campo client del documento que estamos generando.\n",
    "\n",
    "Grok ha evolucionado bastante y ya tiene predefinidas las expresiones regulares de la mañoría de patrones que buscamos nomalmente en una cadena de log.\n",
    "\n",
    "Para ver como funciona vamos a utilizar una de las Dev Tools de Kibana, Grok Debugger, que nos permite crear y depurar nuestro script de gork.\n",
    "\n",
    "Primero entremos en el Grok Debugger:\n",
    "\n",
    "* Kibana > menú > Dev Tools > pestaña Grok Debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a coger una línea del fichero de logs que queremos parsear y la vamos a utilizar como cadena de ejemplo sobre la que aplicar el script que vayamos generando.\n",
    "\n",
    "`\n",
    "89.47.79.75 - - [22/Jan/2019:03:59:11 +0330] \"GET /static/images/search-category-arrow.png HTTP/1.1\" 200 217 \"https://znbl.ir/static/bundle-bundle_site_head.css\" \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"\n",
    "`\n",
    "\n",
    "Y vamos a ir aplicando diferentes expresiones para ver como parsear y extraer toda la información que podamos de la linea del log.\n",
    "\n",
    "Por ejemplo, lo primero que reconocemos en esta es la dirección IP del cliente que realiza la petción, si introduccimos el siguieten patrón %{IP:destination} y clickamos en \"simulate\" veremos que genera un documento con el campo \"destination\" con valor la IP que figura en la línea de log. \n",
    "La expresión regular que está aplicando es la siguiente:\n",
    "* IP (?:%{IPV6}|%{IPV4})\n",
    "\n",
    "Puede ser que en la línea de log venga una IP o un Hostname Si nos queremos asegurar que pueda extarer tanto IPs como nombres de dominio, podemos usar el patrón %{IPORHOST:destination.domain}. \n",
    "\n",
    "1. Ejecuta el pattern %{IPORHOST:destination.domain} para ver como funciona.\n",
    "\n",
    "Bien, vamos ahora a intentar extraer la fehca y la hora en la que se realizo la petición que sería el siguiente dato que somos capaces de reconocer. Para ello vamos a utilizar el patrón predefinido HTTPDATE:\n",
    "* HTTPDATE: %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}\n",
    "\n",
    "`\n",
    "%{IPORHOST:source.address} - - \\[%{HTTPDATE:apache.access.time}\\] \n",
    "`\n",
    "\n",
    "1. Ejecuta este patrón en Grok Debugger\n",
    "\n",
    "Como la fecha viene entre dos corchetes, se lo tenemos que indicar a la expresión y además como son caracteres reservados del lenguaje Grok los tenemos que escapar.\n",
    "\n",
    "Vamos ahora a extraer la información de la petición, para ello usaremos los siguientes patrones:\n",
    "\n",
    "* WORD: \\b\\w+\\b\n",
    "* DATA: .*?\n",
    "* NUMBER: (?:%{BASE10NUM})\n",
    "\n",
    "El escript quedaría de la siguiente manera:\n",
    "\n",
    "`\n",
    "%{IPORHOST:source.address} - - \\[%{HTTPDATE:apache.access.time}\\] \\\"(?:%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}|-)?\\\"\n",
    "`\n",
    "\n",
    "1. Ejecuta este patrón en Grok Debugger\n",
    "\n",
    "De esta manera seguiríamos parseando y extrayendo información de la cadena de log.\n",
    "\n",
    "En este enlace tienes un listado con todos los patrones predefinidos por Elastic: https://github.com/elastic/elasticsearch/blob/main/libs/grok/src/main/resources/patterns/legacy/grok-patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro processor interesante que vamos a utilizar es geoip que nos permite buscar en una base de datos de IPs la dirección que hemos parseado y buscar sus coordenadas geográficas. De esta forma nos sólo estructuramos la información sino que también la aumentamos y enriquecemos con más información relacionada.\n",
    "\n",
    "Una vez que ya hemos visto como funcionan los processor que vamos a utilizar, creamos el pipeline ejecutando la siguiente sentencia: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true}"
     ]
    }
   ],
   "source": [
    "!curl -X PUT http://elasticsearch:9200/_ingest/pipeline/web-logs -H 'Content-Type: application/json' -d ' \\\n",
    "{ \\\n",
    "  \"description\" : \"Pipeline for parsing Apache HTTP Server access logs. Requires the geoip and user_agent plugins.\", \\\n",
    "  \"processors\" : [ \\\n",
    "    { \\\n",
    "      \"set\": { \\\n",
    "        \"field\": \"event.original\", \\\n",
    "        \"value\": \"{{message}}\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"set\" : { \\\n",
    "        \"field\" : \"event.ingested\", \\\n",
    "        \"value\" : \"{{_ingest.timestamp}}\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"grok\" : { \\\n",
    "        \"patterns\" : [ \\\n",
    "          \"%{IPORHOST:destination.domain} %{IPORHOST:source.ip} - %{DATA:user.name} \\\\[%{HTTPDATE:apache.access.time}\\\\] \\\"(?:%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}|-)?\\\" %{NUMBER:http.response.status_code:long} (?:%{NUMBER:http.response.body.bytes:long}|-)( \\\"%{DATA:http.request.referrer}\\\")?( \\\"%{DATA:user_agent.original}\\\")?\", \\\n",
    "          \"%{IPORHOST:source.address} - %{DATA:user.name} \\\\[%{HTTPDATE:apache.access.time}\\\\] \\\"(?:%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}|-)?\\\" %{NUMBER:http.response.status_code:long} (?:%{NUMBER:http.response.body.bytes:long}|-)( \\\"%{DATA:http.request.referrer}\\\")?( \\\"%{DATA:user_agent.original}\\\")?\", \\\n",
    "          \"%{IPORHOST:source.address} - %{DATA:user.name} \\\\[%{HTTPDATE:apache.access.time}\\\\] \\\"-\\\" %{NUMBER:http.response.status_code:long} -\", \\\n",
    "          \"\\\\[%{HTTPDATE:apache.access.time}\\\\] %{IPORHOST:source.address} %{DATA:apache.access.ssl.protocol} %{DATA:apache.access.ssl.cipher} \\\"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\\\" (-|%{NUMBER:http.response.body.bytes:long})\" \\\n",
    "        ], \\\n",
    "        \"ignore_missing\" : true, \\\n",
    "        \"field\" : \"message\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"remove\" : { \\\n",
    "        \"field\" : \"message\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"set\" : { \\\n",
    "        \"field\" : \"event.kind\", \\\n",
    "        \"value\" : \"event\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"set\" : { \\\n",
    "        \"field\" : \"event.category\", \\\n",
    "        \"value\" : \"web\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"set\" : { \\\n",
    "        \"value\" : \"success\", \\\n",
    "        \"if\" : \"ctx?.http?.response?.status_code != null && ctx.http.response.status_code < 400\", \\\n",
    "        \"field\" : \"event.outcome\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"set\" : { \\\n",
    "        \"field\" : \"event.outcome\", \\\n",
    "        \"value\" : \"failure\", \\\n",
    "        \"if\" : \"ctx?.http?.response?.status_code != null && ctx.http.response.status_code > 399\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"grok\" : { \\\n",
    "        \"field\" : \"source.address\", \\\n",
    "        \"ignore_missing\" : true, \\\n",
    "        \"patterns\" : [ \\\n",
    "          \"^(%{IP:source.ip}|%{HOSTNAME:source.domain})$\" \\\n",
    "        ] \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"rename\" : { \\\n",
    "        \"target_field\" : \"event.created\", \\\n",
    "        \"field\" : \"@timestamp\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"date\" : { \\\n",
    "        \"ignore_failure\" : true, \\\n",
    "        \"field\" : \"apache.access.time\", \\\n",
    "        \"target_field\" : \"@timestamp\", \\\n",
    "        \"formats\" : [ \\\n",
    "          \"dd/MMM/yyyy:H:m:s Z\" \\\n",
    "        ] \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"remove\" : { \\\n",
    "        \"field\" : \"apache.access.time\", \\\n",
    "        \"ignore_failure\" : true \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"user_agent\" : { \\\n",
    "        \"field\" : \"user_agent.original\", \\\n",
    "        \"ignore_failure\" : true \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"geoip\" : { \\\n",
    "        \"field\" : \"source.ip\", \\\n",
    "        \"target_field\" : \"source.geo\", \\\n",
    "        \"ignore_missing\" : true \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"geoip\" : { \\\n",
    "        \"target_field\" : \"source.as\", \\\n",
    "        \"properties\" : [ \\\n",
    "          \"asn\", \\\n",
    "          \"organization_name\" \\\n",
    "        ], \\\n",
    "        \"ignore_missing\" : true, \\\n",
    "        \"database_file\" : \"GeoLite2-ASN.mmdb\", \\\n",
    "        \"field\" : \"source.ip\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"rename\" : { \\\n",
    "        \"field\" : \"source.as.asn\", \\\n",
    "        \"target_field\" : \"source.as.number\", \\\n",
    "        \"ignore_missing\" : true \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"rename\" : { \\\n",
    "        \"ignore_missing\" : true, \\\n",
    "        \"field\" : \"source.as.organization_name\", \\\n",
    "        \"target_field\" : \"source.as.organization.name\" \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"set\" : { \\\n",
    "        \"field\" : \"tls.cipher\", \\\n",
    "        \"value\" : \"{{apache.access.ssl.cipher}}\", \\\n",
    "        \"ignore_empty_value\" : true \\\n",
    "      } \\\n",
    "    }, \\\n",
    "    { \\\n",
    "      \"script\" : { \\\n",
    "        \"lang\" : \"painless\", \\\n",
    "        \"if\" : \"ctx?.apache?.access?.ssl?.protocol != null\", \\\n",
    "        \"source\" : \"def parts = ctx.apache.access.ssl.protocol.toLowerCase().splitOnToken(\\\"v\\\"); if (parts.length != 2) {\\n  return;\\n} if (parts[1].contains(\\\".\\\")) {\\n  ctx.tls.version = parts[1];\\n} else {\\n  ctx.tls.version = parts[1] + \\\".0\\\";\\n} ctx.tls.version_protocol = parts[0];\" \\\n",
    "      } \\\n",
    "    } \\\n",
    "  ], \\\n",
    "  \"on_failure\" : [ \\\n",
    "    { \\\n",
    "      \"set\" : { \\\n",
    "        \"field\" : \"error.message\", \\\n",
    "        \"value\" : \"{{ _ingest.on_failure_message }}\" \\\n",
    "      } \\\n",
    "    } \\\n",
    "  ] \\\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprueba que se ha generado correctamente entrando en la sección de Ingest Pipelines y buscando el pipeline \"web-logs\" que acabamos de crear:\n",
    "\n",
    "* Kibana > menú > Management > Stack Management > Ingest > Ingest Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Configuramos Logshtash\n",
    "\n",
    "Ya sólo nos queda configurar Logshtash para que lea los ficheros de la ruta donde guarda los logs el servidor Apache, indicar los filtros que queremos que aplique y por último decirle en que índice de Elasticsearch queremos que deje los datos.\n",
    "\n",
    "Por cada instancia o servicio de Logstahs que levantemos podemos ejecutar uno o más pipelines de ingesta. Cada uno de stos procesos se define en su respectivo fichero. Dejaremos estos ficheros en la carpeta pipeline de logstash, en nuestro caso en la ruta '/usr/share/logstash/pipeline/'.\n",
    "\n",
    "Vamos a definir el pipeline de ingesta para nuestro ejemplo. Lo puedes encontrar aquí: \n",
    "http://127.0.0.1:8889/edit/work/data/elasticsearch/web_logs/pipeline/web-logs-logstash.conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El pipeline tiene tres prates:\n",
    "\n",
    "* **input** -> indicamos de donde queremos recuparar los dastos que queremos ingestar.\n",
    "\n",
    "`\n",
    "input {\n",
    "    path => \"/tmp/data/*\"\n",
    "}\n",
    "`\n",
    "\n",
    "Vamos a leer los ficheros de log que se vayan creando el la ruta '/tmp/data/'. \n",
    "\n",
    "\n",
    "* **filter** -> definimos la cadena de processors a ejecutar sobre los datos originales para trandformalos y darles estructura.\n",
    "\n",
    "`\n",
    "filter {\n",
    "  mutate {\n",
    "    remove_field => [\"host\", \"@version\"]\n",
    "  }\n",
    "}\n",
    "`\n",
    "\n",
    "Puesto que hemos creado un pipeline de ingesta, solo le vamos a indicar a Logstash eliminar los campos 'host' y '@version'.\n",
    "\n",
    "\n",
    "* **output** -> indicamos donde queremos dejar o insertar los datos adquiridos.\n",
    "\n",
    "`\n",
    "output {\n",
    "  stdout {\n",
    "    codec => dots {}\n",
    "  }\n",
    "\n",
    "  elasticsearch {\n",
    "    hosts => \"localhost:9200\"\n",
    "    index => \"web-logs\"\n",
    "    pipeline => \"web-logs\"\n",
    "  }\n",
    "}\n",
    "`\n",
    "\n",
    "Por un lado vamos a imprmir por panta un punto por cada línea de log parseada. Por otro lado vamos a enviar el documento creado a partir de cada línea de log leída a Elasticsearch. Para ello tenemos que indicar los parámetros de configuración de Elasticsearch, la url o urls de los hosts del cluster de Elasticsearch, el índice donde insertar los docuentos y por último el pipeline a ejecutar a la hora de insertar los datos (este prámetro es opcional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Levantar Logstash\n",
    "\n",
    "En nuestro caso vamos a utilizar una imagen de docker para ejecutar Logstash:\n",
    "\n",
    "* Para que pueda encontrar el servicio de Elasticsearch vamos a añadir el conteneror a la red de nuestro laboratorio, `--network=datahack-nosql_default`.\n",
    "* Montamos el volumen donde se encuentra nuestro fichero con el pipeline y los referenciamos a la carpeta del contenedor donde Logstash espera encontrar esa configuración, `-v /Users/rgarrote/desarrollo/datahack-nosql/work/data/elasticsearch/web_logs/pipeline/:/usr/share/logstash/pipeline/`.\n",
    "* Montamos el volumen donde dejaremos los ficheros de log a parsear. `-v /Users/rgarrote/desarrollo/datahack-nosql/work/data/elasticsearch/web_logs/data/:/tmp/data/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --rm -it --network=datahack-nosql_default \\\n",
    "    -v /Users/rgarrote/desarrollo/datahack-nosql/work/data/elasticsearch/web_logs/pipeline/:/usr/share/logstash/pipeline/ \\\n",
    "    -v /Users/rgarrote/desarrollo/datahack-nosql/work/data/elasticsearch/web_logs/data/:/tmp/data/ \\\n",
    "docker.elastic.co/logstash/logstash:8.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 5: Comprobar que el proceso se está realizando correctamente\n",
    "\n",
    "1. Comprueba en Kibana que se ha ceado el íncide web-logs.\n",
    "2. Desde la sección de Index Management averigua cuantas líneas de log se han insertado en el índice de Elasticsearch.\n",
    "3. Comprueba que el mapping creado del índice corresponde al del template que hemos creado.\n",
    "4. Realiza una consulta de con una muestra de 10 documentos para comprobar que los datos se están parseando e insertando correctamente en Elasticsearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
