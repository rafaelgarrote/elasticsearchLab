{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Creación de un pipeline de Ingesta\n",
    "\n",
    "A la hora de trabajar en un caso de uso real es muy común que los datos que quermos adquirir e insertar en Elasticsearch no tengan un formato correcto, bien porque sean fuentes de datos semiestructuradas o directamente no estructuradas, o bien porque queramos enriquecer los datos con ciertas operaciones para calcular inforamción extra, como por ejemplo extraer la localización de una petición a un servidor web a partir de la dirección IP de origen.\n",
    "\n",
    "Para facilitar todas estas tareas Elasticsearch nos ofrece diferentes mecanismos, el que vamos a ver en esta práctica es el uso de pipelines de ingesta. Otra ventaja de utilizar este mecanismo, es que no necesitamos utilizar otras tecnologías diferentes a Elasticsearch y por tanto ayuda a simplificar nuestra arquitectura de la infraestructura de datos.\n",
    "\n",
    "Los pipelines de ingesta nos van a permitir ejecutar transformaciones comunes sobre los datos antes de indexarlos, como por ejemplo eliminar campos, extraer valores de textos o enriquecer los datos.\n",
    "\n",
    "Un pipeline consiste en una serie de tareas o tasks configurables denominados processors. Cada processor se ejecuta secuencialmente ejecutando los cambios especificados sobre los documentos de entrada. Los documentos resultantes serán la fuente de entrada del siguiente proccesor configurado. Una vez ejecutados todos los processors, Elasticsearch añadirá los documentos transformados en el índice que indiquemos en el pipeline.\n",
    "\n",
    "<img src=\"../../images/els/ingest-process.svg\" alt=\"ingest process\"/>\n",
    "\n",
    "Los pipelines de ingesta ofrecen una solución ligera para realizar tareas de transformación y manipulación de datos cuando no disponemos (o no nos interesa disponer) de una erramienta de ETL. Puesto que los pipelines se ejecutan en los nodos de Elasticsearch, se puede escalar la infraestructura necesara de forma sencilla como parte del cluster.\n",
    "\n",
    "En el siguiente enlace podrás encontrar un listado con los processors de los que dispone Elasticsearch: https://www.elastic.co/guide/en/elasticsearch/reference/8.0/processors.html.\n",
    "\n",
    "En esta práctica vamos a ver como crear, configurar y ejecutar un pipeline de ingesta de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creamos un pipeline de ingesta indicando los processors y su orden de ejecución\n",
    "\n",
    "Más adelante veremos los tipos de processors que existen. Por ahora sólo fíjate en la sintaxis.\n",
    "\n",
    "En este caso creamos un pipeline que sólo tiene un step definido con el processor set que añade un nuevo campo en el documento a indexar, \"environment\" y le asigna el valor \"production\" independientemente del contenido del documento.  \n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/logs-add-tag\n",
    "{\n",
    "  \"description\": \"Adds a static tag for the environment the log originates from\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"field\": \"environment\",\n",
    "        \"value\": \"production\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vamos a probar el pipeline de ingesta ejecutando algunos test a través de él\n",
    "\n",
    "Para ello utilizamos el método _simulate\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/logs-add-tag/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"host.os\": \"macOS\",\n",
    "        \"source.ip\": \"10.22.11.89\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "En la salida veremos el nuevo campo añadido a demás de los anteriores:\n",
    "\"environment\": \"production\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesar los documentos con el pipeline de ingesta\n",
    "\n",
    "Para ello tenemos tes pociones:\n",
    "\n",
    "### 3a. Especificando el pipeline de ingest al indexar un documento \n",
    "\n",
    "`\n",
    "POST log-index/_doc?pipeline=logs-add-tag\n",
    "{\n",
    "  \"host.os\": \"windows 10\",\n",
    "  \"source.ip\": \"113.121.143.90\"\n",
    "}\n",
    "`\n",
    "\n",
    "Vamos a comprobar que el pipeline de ingesta se ha ejecutado al insertar el documento. Para ello buscamos el documento que acabamos de insertar y comprobamos que el documento contiene el campo \"environment\":\n",
    "\n",
    "`\n",
    "POST log-index/_search\n",
    "{\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"host.os.keyword\" : \"windows 10\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "`\n",
    "\n",
    "### 3b. Indicando el pipeline a usar en una operación de tipo bulk\n",
    "\n",
    "`\n",
    "POST _bulk\n",
    "{ \"index\" : { \"_index\" : \"log-index\", \"_id\" : \"1\",\"pipeline\": \"logs-add-tag\" } }\n",
    "{ \"host.os\" : \"windows 7\", \"source.ip\": \"10.0.0.1\" }\n",
    "`\n",
    "\n",
    "Comprobamos igual que antes que se ha añadido el campo \"environment\" como resultado de la ejecución del pipeline:\n",
    "\n",
    "`\n",
    "POST log-index/_search\n",
    "{\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"host.os.keyword\" : \"windows 7\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "`\n",
    "\n",
    "### 3c. Especificando el pipeline de ingesta por defecto como un setting de un índice\n",
    "\n",
    "Set the default pipeline for an index as follows:\n",
    " \n",
    "`\n",
    "PUT log-index/_settings\n",
    "{ \"index.default_pipeline\": \"logs-add-tag\" }\n",
    "`\n",
    "\n",
    "Note that the setting can also be set using an index template, just like any other index setting.\n",
    "Index a document to the index without specifying any pipeline query parameters, and then search the index to confirm the document was tagged as expected:\n",
    "\n",
    "`\n",
    "POST log-index/_doc/\n",
    "{\n",
    "  \"host.os\": \"linux\",\n",
    "  \"source.ip\": \"10.10.10.1\"\n",
    "}\n",
    "`\n",
    "\n",
    "Volvemos a comprobar que se ha ejecutado de forma correcta el pipeline:\n",
    "\n",
    "`\n",
    "POST log-index/_search\n",
    "{\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"host.os.keyword\" : \"linux\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gestionar los errores de ejecución de un pipeline\n",
    "\n",
    "Los processors de un pipeline de ingesta se ejecutan secuencialmente. Por defecto si un processor falla en su ejecución, automáticamente se para el proceso de ingesta para el documento que se estaba ingestando.\n",
    "\n",
    "Para gestionar los errores de ejecución, y modificar el comportamiento por defecto, tenemos tres alternativas que se pueden utilizar de forma simultánea:\n",
    "\n",
    "#### 4a. Ignorar el error\n",
    "Para ello utilizaremos el campo \"ignore_failure\" al definir el processor asignándole el valor \"true\". De esta manera, el porcessor ingorará el fallo y seguirá ejecutando los siguientes processors definidos.\n",
    "\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"rename\": {\n",
    "        \"description\": \"Rename 'provider' to 'cloud.provider'\",\n",
    "        \"field\": \"provider\",\n",
    "        \"target_field\": \"cloud.provider\",\n",
    "        \"ignore_failure\": true\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "#### 4b. Especificar una lista de processors a ejecutar inmediatamene después de que un processor falle\n",
    "Utilizando el campo \"on_failure\" de un processor pordemos definir la secuencia de processors a ejecutar cuando este falla. De esta forma podemos tratarán el error y por ejemplo añadir el campo \"error.message\" con el mensaje que indique el error producido:\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"rename\": {\n",
    "        \"description\": \"Rename 'provider' to 'cloud.provider'\",\n",
    "        \"field\": \"provider\",\n",
    "        \"target_field\": \"cloud.provider\",\n",
    "        \"on_failure\": [\n",
    "          {\n",
    "            \"set\": {\n",
    "              \"description\": \"Set 'error.message'\",\n",
    "              \"field\": \"error.message\",\n",
    "              \"value\": \"Field 'provider' does not exist. Cannot rename to 'cloud.provider'\",\n",
    "              \"override\": false\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Podemos anidar tantos processors como queramos utilizando el campo \"on_failure\"\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"rename\": {\n",
    "        \"description\": \"Rename 'provider' to 'cloud.provider'\",\n",
    "        \"field\": \"provider\",\n",
    "        \"target_field\": \"cloud.provider\",\n",
    "        \"on_failure\": [\n",
    "          {\n",
    "            \"set\": {\n",
    "              \"description\": \"Set 'error.message'\",\n",
    "              \"field\": \"error.message\",\n",
    "              \"value\": \"Field 'provider' does not exist. Cannot rename to 'cloud.provider'\",\n",
    "              \"override\": false,\n",
    "              \"on_failure\": [\n",
    "                {\n",
    "                  \"set\": {\n",
    "                    \"description\": \"Set 'error.message.multi'\",\n",
    "                    \"field\": \"error.message.multi\",\n",
    "                    \"value\": \"Document encountered multiple ingest errors\",\n",
    "                    \"override\": true\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "#### 4c. Definir una secuencia de processors general para todo el pipeline\n",
    "\n",
    "Podemos definir una secuencia de processors general a ejecutar en el caso de que alguno de los processors del pipeline falle.\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [ ... ],\n",
    "  \"on_failure\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"description\": \"Index document to 'failed-<index>'\",\n",
    "        \"field\": \"_index\",\n",
    "        \"value\": \"failed-{{{ _index }}}\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Podemos encontrar información adicional del fallo en los metadatos del documento que son accesibles en el bloque \"on_failure\": on_failure_message, on_failure_processor_type, on_failure_processor_tag y on_failure_pipeline\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [ ... ],\n",
    "  \"on_failure\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"description\": \"Record error information\",\n",
    "        \"field\": \"error_information\",\n",
    "        \"value\": \"Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casos de uso más comunes\n",
    "\n",
    "Vamos a ver algunos ejemplos a través de casos de uso muy comunes que nos podemos encontrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parsear los valores de los campos y extraer valores útiles para nuevos campos.\n",
    "\n",
    "Processors utilizados:\n",
    "* dissect: https://www.elastic.co/guide/en/elasticsearch/reference/current/dissect-processor.html#dissect-processor\n",
    "* lowercase: https://www.elastic.co/guide/en/elasticsearch/reference/current/lowercase-processor.html#lowercase-processor\n",
    "* remove: https://www.elastic.co/guide/en/elasticsearch/reference/current/remove-processor.html#remove-processor\n",
    "\n",
    "Vamos a suponer que los documentos a insertar contienen para el campo \"message\" cadenas de texto con el siguiente formato:\n",
    "\n",
    "`\n",
    "\"10:12:05 HTTP Monitor production is in GREEN state\"\n",
    "\"10:12:05 HTTP Monitor production is in RED state\"\n",
    "`\n",
    "\n",
    "En este caso hacen referencia a líneas de un fichero de log.\n",
    "\n",
    "Con el processor dissect podemos parsear estas cadenas y extraer ciertos valores e insertarlos en nuevos campos en el documento de entrada. \n",
    "\n",
    "Por ejemplo, vamos a extraer la hora el nombre del monitor y el estado para insertarlo en los respectivos campos:\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-one\n",
    "{\n",
    "  \"description\": \"Parse and extract log useful fields\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"dissect\": {\n",
    "        \"field\": \"message\",\n",
    "        \"pattern\": \"%{time} HTTP Monitor %{monitor.name} is in %{monitor.state} state\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Probemos el pipeline utilizando la función sumulate:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-one/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"message\": \"10:12:05 HTTP Monitor production is in GREEN state\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Para completar el pipeline, vamos a convertir en minúsculas el nuevo campo \"monitor.state\" utilizando el processor lowercase y a eliminar el campo original \"message\" con el processor remove. Lo haremos modificando el pipeline anteriror:\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-one\n",
    "{\n",
    "  \"description\": \"Parse and extract log useful fields\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"dissect\": {\n",
    "        \"field\": \"message\",\n",
    "        \"pattern\": \"%{time} HTTP Monitor %{monitor.name} is in %{monitor.state} state\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"lowercase\": {\n",
    "        \"field\": \"monitor.state\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"remove\": {\n",
    "        \"field\": \"message\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobamos que el pipeline se ha modificado correctamente:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-one/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"message\": \"10:12:05 HTTP Monitor production is in GREEN state\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tagear un documento en función del valor de un campo de documento base\n",
    "\n",
    "En este caso vamos a utilizar sentencias condicionales para crear un nuevo campo y asignarle un valor en función del contenido de un campo del documento base.\n",
    "\n",
    "Processors utilizados:\n",
    "* set: https://www.elastic.co/guide/en/elasticsearch/reference/current/set-processor.html#set-processor\n",
    "\n",
    "\n",
    "Suponemos que tenemos los siguientes documentos base:\n",
    "\n",
    "`\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"subnet\": \"CTS-01\",\n",
    "  \"classification\": \"secret\"\n",
    "}\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"subnet\": \"ATT-01\",\n",
    "  \"classification\": \"unclassified\"\n",
    "}\n",
    "`\n",
    "\n",
    "El processor set utiliza un script para comprobar los valores de los campos \"classification\" y \"subnet\", si los campos cumplen la condición del script entonces se tagean con el vaor \"protected\" añadiendo dicho valor al campo \"tag\".\n",
    "\n",
    "El campo \"if\" utiliza painless para definir la condición.\n",
    "\n",
    "Nota: \"ctx\" hace referencia al documento base.\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-two\n",
    "{\n",
    "  \"description\": \"Tag a document based on field value\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"if\": \"ctx.classification=='secret' && ctx.subnet=='CTS-01'\",\n",
    "        \"field\": \"tag\",\n",
    "        \"value\": \"protected\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobemos que el pipeline funciona correctamente:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-two/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"subnet\": \"CTS-01\",\n",
    "        \"classification\": \"secret\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"subnet\": \"ATT-01\",\n",
    "        \"classification\": \"unclassified\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descartar eventos de log no deseados basándose en los valores de los campos de tal forma que no se ingesten en el índice.\n",
    "\n",
    "En este caso vamos a descartar documentos en función del contenido de uno o varios campos. Para ello vamos primero a crear un uevo campo \"tag\" con el valor \"drop\" si cumple la condición descrita por el processor script y después vamos a descartar los documentos cuyo campo \"tag\" contengan el valor \"drop\" utilizando el porcessor drop.\n",
    "\n",
    "Processors utilizados:\n",
    "* script: https://www.elastic.co/guide/en/elasticsearch/reference/current/drop-processor.html#drop-processor\n",
    "* drop: https://www.elastic.co/guide/en/elasticsearch/reference/current/drop-processor.html#drop-processor\n",
    "\n",
    "El formato de los ducumentos base es el siguiente:\n",
    "\n",
    "`\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"subnet\": \"CTS-01\",\n",
    "  \"event_code\": \"AS-32\"\n",
    "}\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"subnet\": \"ATT-01\",\n",
    "  \"event_code\": \"AS-29\"\n",
    "}\n",
    "`\n",
    "\n",
    "Creamos el pipeline que procese los documentos y descarte aquellos que tienen un event_code no permitido:\n",
    "El processor script permite utilizar varios lenguajes de scripting:\n",
    "* painless: Por defecto sino se indica el calpo \"lang\". Leguaje de scripting propio de Elasticsearch. https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html#modules-scripting-painless\n",
    "* expression: Lenguaje de scripting de Lucene. https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-expression.html#modules-scripting-expression\n",
    "* mustache: Pensado para templates. https://mustache.github.io/\n",
    "* Java: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html#modules-scripting-painless\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-three\n",
    "{\n",
    "  \"description\": \"Dop document based on field value\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"script\": {\n",
    "        \"lang\": \"painless\",\n",
    "        \"source\": \"\"\"\n",
    "          def disallowedCodes = [\"AS-29\",\"BA-23\"];\n",
    "          if (disallowedCodes.contains(ctx.event_code)) {\n",
    "            ctx.tag = \"drop\";\n",
    "          }\n",
    "          \"\"\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "      \"drop\": {\n",
    "        \"if\": \"ctx.tag == 'drop'\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobemos el correcto funcionamiento del pipeline que acabamos de definir:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-three/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"subnet\": \"CTS-01\",\n",
    "        \"event_code\": \"AS-32\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"subnet\": \"ATT-01\",\n",
    "        \"event_code\": \"AS-29\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Enrutar e indexar documentos en el ídice correcto de Elasticsearch basándose en el valor de los campos del documento.\n",
    "\n",
    "Para indicar en que índice se insertará un documento, vamos a sobreescribir el campo \"_index\" que le indica a elasticsearch que índice es el que debe utilizar para indexar un documento.\n",
    "\n",
    "Processors utilizados:\n",
    "* set: https://www.elastic.co/guide/en/elasticsearch/reference/current/set-processor.html#set-processor\n",
    "\n",
    "Para ello utilizaremos el processor set y asignaremos al campo \"_index\" el resultado de concatenar el valor del campo \"application\" del documento base con el valor del campo \"environment\" del documento base. De esta forma conseguiremos tener separados en diferentes índices los documentos de cada aplicación y por entorno.\n",
    "\n",
    "Los documentos base tienen el siguiente formato:\n",
    "\n",
    "`\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"application\": \"apache\"\n",
    "}\n",
    "{\n",
    "  \"environment\": \"dev\",\n",
    "  \"application\": \"apache\"\n",
    "}\n",
    "`\n",
    "\n",
    "\n",
    "Creamos el pipeline:\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-four\n",
    "{\n",
    "  \"description\": \"Route document into correct index based on field value\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"field\": \"_index\",\n",
    "        \"value\": \"{{application}}-{{environment}}\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobemos el correcto funcionamiento del pipeline:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-four/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"application\": \"apache\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"dev\",\n",
    "        \"application\": \"apache\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Enmascarar información sensible almacenada en los valores de los campos del documento.\n",
    "\n",
    "Otro caso de uso muy util es poder enmascarar información sensible como en este caso los números de tarjetas de crédito. \n",
    "\n",
    "Processors utilizados:\n",
    "* gsub: https://www.elastic.co/guide/en/elasticsearch/reference/current/gsub-processor.html#gsub-processor\n",
    "\n",
    "\n",
    "Vamos a suponer el siguiente formato de documento base:\n",
    "\n",
    "`\n",
    "{\n",
    "  \"message\": \"Customer A1121 paid with 5555555555554444\"\n",
    "}\n",
    "{\n",
    "  \"message\": \"Customer A1122 paid with 378282246310005\"\n",
    "}\n",
    "`\n",
    "\n",
    "Para ello vamos a definir una expresión regular que sea capaz de extraer el número de una tarjeta de credito y lo utilizaremos con el processor gsub que permite modificar una cadena aplicando expresiones regulares.\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-five\n",
    "{\n",
    "  \"description\": \"Strip sensitive information from the fields in documents\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"gsub\": {\n",
    "        \"field\": \"message\",\n",
    "        \"pattern\": \"\\\\b(?:3[47]\\\\d|(?:4\\\\d|5[1-5]|65)\\\\d{2}|6011)\\\\d{12}\\\\b\",\n",
    "        \"replacement\": \"xxxx-xxxx-xxxx-xxxx\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Probamos el funcionamiento del pipeline:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-five/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"message\": \"Customer A1121 paid with 5555555555554444\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"message\": \"Customer A1122 paid with 378282246310005\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Enriquecer la el campo que contiene una dirección IP pública con información geográfica.\n",
    "\n",
    "También podemos utilizar el processor geoip para buscar las coordenadas geográficas de una IP y enriquecer el documento base con esta información.\n",
    "\n",
    "Processors utilizados:\n",
    "* geoip: https://www.elastic.co/guide/en/elasticsearch/reference/current/geoip-processor.html#geoip-processor\n",
    "* rename: https://www.elastic.co/guide/en/elasticsearch/reference/current/rename-processor.html#rename-processor\n",
    "\n",
    "El formato del documento base que utilizaremos es:\n",
    "`\n",
    "{\n",
    "  \"source_ip\": \"194.121.12.154\"\n",
    "}\n",
    "`\n",
    "\n",
    "Definimos el pipeline para ello primero extraemos las coordenadas geográficas a partir del campo \"soruce_ip\" del documento base y dejamos los valores obtenidos en el campo \"source.geo\". Después renombramos el campo source_ip por \"source.ip\" para embeberlo dendro del documento source junto con la información geográfica. \n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-six\n",
    "{\n",
    "  \"description\": \"Enrich the public IP address fields with geo-location information\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"geoip\": {\n",
    "        \"field\": \"source_ip\",\n",
    "        \"target_field\": \"source.geo\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"rename\": {\n",
    "        \"field\": \"source_ip\",\n",
    "        \"target_field\": \"source.ip\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobamos su funcionamiento. Puedes probar con tu IP publica.\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-six/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"source_ip\": \"194.121.12.154\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
