{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creamos un pipeline de ingesta indicando los processors y su orden de ejecución\n",
    "\n",
    "Más adelante veremos los tipos de processors que existen. Por ahora sólo fíjate en la sintaxis.\n",
    "\n",
    "En este caso creamos un pipeline que sólo tiene un step definido con el processor set que añade un nuevo campo en el documento a indexar, \"environment\" y le asigna el valor \"production\" independientemente del contenido del documento.  \n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/logs-add-tag\n",
    "{\n",
    "  \"description\": \"Adds a static tag for the environment the log originates from\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"field\": \"environment\",\n",
    "        \"value\": \"production\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vamos a probar el pipeline de ingesta ejecutando algunos test a través de él\n",
    "\n",
    "Para ello utilizamos el método _simulate\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/logs-add-tag/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"host.os\": \"macOS\",\n",
    "        \"source.ip\": \"10.22.11.89\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "En la salida veremos el nuevo campo añadido a demás de los anteriores:\n",
    "\"environment\": \"production\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesar los documentos con el pipeline de ingesta\n",
    "\n",
    "Para ello tenemos tes pociones:\n",
    "\n",
    "### 3a. Especificando el pipeline de ingest al indexar un documento \n",
    "\n",
    "`\n",
    "POST log-index/_doc?pipeline=logs-add-tag\n",
    "{\n",
    "  \"host.os\": \"windows 10\",\n",
    "  \"source.ip\": \"113.121.143.90\"\n",
    "}\n",
    "`\n",
    "\n",
    "Vamos a comprobar que el pipeline de ingesta se ha ejecutado al insertar el documento. Para ello buscamos el documento que acabamos de insertar y comprobamos que el documento contiene el campo \"environment\":\n",
    "\n",
    "`\n",
    "POST log-index/_search\n",
    "{\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"host.os.keyword\" : \"windows 10\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "`\n",
    "\n",
    "### 3b. Indicando el pipeline a usar en una operación de tipo bulk\n",
    "\n",
    "`\n",
    "POST _bulk\n",
    "{ \"index\" : { \"_index\" : \"log-index\", \"_id\" : \"1\",\"pipeline\": \"logs-add-tag\" } }\n",
    "{ \"host.os\" : \"windows 7\", \"source.ip\": \"10.0.0.1\" }\n",
    "`\n",
    "\n",
    "Comprobamos igual que antes que se ha añadido el campo \"environment\" como resultado de la ejecución del pipeline:\n",
    "\n",
    "`\n",
    "POST log-index/_search\n",
    "{\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"host.os.keyword\" : \"windows 7\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "`\n",
    "\n",
    "### 3c. Especificando el pipeline de ingesta por defecto como un setting de un índice\n",
    "\n",
    "Set the default pipeline for an index as follows:\n",
    " \n",
    "`\n",
    "PUT log-index/_settings\n",
    "{ \"index.default_pipeline\": \"logs-add-tag\" }\n",
    "`\n",
    "\n",
    "Note that the setting can also be set using an index template, just like any other index setting.\n",
    "Index a document to the index without specifying any pipeline query parameters, and then search the index to confirm the document was tagged as expected:\n",
    "\n",
    "`\n",
    "POST log-index/_doc/\n",
    "{\n",
    "  \"host.os\": \"linux\",\n",
    "  \"source.ip\": \"10.10.10.1\"\n",
    "}\n",
    "`\n",
    "\n",
    "Volvemos a comprobar que se ha ejecutado de forma correcta el pipeline:\n",
    "\n",
    "`\n",
    "POST log-index/_search\n",
    "{\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"host.os.keyword\" : \"linux\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gestionar los errores de ejecución de un pipeline\n",
    "\n",
    "Los processors de un pipeline de ingesta se ejecutan secuencialmente. Por defecto si un processor falla en su ejecución, automáticamente se para el proceso de ingesta para el documento que se estaba ingestando.\n",
    "\n",
    "Para gestionar los errores de ejecución, y modificar el comportamiento por defecto, tenemos tres alternativas que se pueden utilizar de forma simultánea:\n",
    "\n",
    "#### 4a. Ignorar el error\n",
    "Para ello utilizaremos el campo \"ignore_failure\" al definir el processor asignándole el valor \"true\". De esta manera, el porcessor ingorará el fallo y seguirá ejecutando los siguientes processors definidos.\n",
    "\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"rename\": {\n",
    "        \"description\": \"Rename 'provider' to 'cloud.provider'\",\n",
    "        \"field\": \"provider\",\n",
    "        \"target_field\": \"cloud.provider\",\n",
    "        \"ignore_failure\": true\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "#### 4b. Especificar una lista de processors a ejecutar inmediatamene después de que un processor falle\n",
    "Utilizando el campo \"on_failure\" de un processor pordemos definir la secuencia de processors a ejecutar cuando este falla. De esta forma podemos tratarán el error y por ejemplo añadir el campo \"error.message\" con el mensaje que indique el error producido:\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"rename\": {\n",
    "        \"description\": \"Rename 'provider' to 'cloud.provider'\",\n",
    "        \"field\": \"provider\",\n",
    "        \"target_field\": \"cloud.provider\",\n",
    "        \"on_failure\": [\n",
    "          {\n",
    "            \"set\": {\n",
    "              \"description\": \"Set 'error.message'\",\n",
    "              \"field\": \"error.message\",\n",
    "              \"value\": \"Field 'provider' does not exist. Cannot rename to 'cloud.provider'\",\n",
    "              \"override\": false\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Podemos anidar tantos processors como queramos utilizando el campo \"on_failure\"\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"rename\": {\n",
    "        \"description\": \"Rename 'provider' to 'cloud.provider'\",\n",
    "        \"field\": \"provider\",\n",
    "        \"target_field\": \"cloud.provider\",\n",
    "        \"on_failure\": [\n",
    "          {\n",
    "            \"set\": {\n",
    "              \"description\": \"Set 'error.message'\",\n",
    "              \"field\": \"error.message\",\n",
    "              \"value\": \"Field 'provider' does not exist. Cannot rename to 'cloud.provider'\",\n",
    "              \"override\": false,\n",
    "              \"on_failure\": [\n",
    "                {\n",
    "                  \"set\": {\n",
    "                    \"description\": \"Set 'error.message.multi'\",\n",
    "                    \"field\": \"error.message.multi\",\n",
    "                    \"value\": \"Document encountered multiple ingest errors\",\n",
    "                    \"override\": true\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "#### 4c. Definir una secuencia de processors general para todo el pipeline\n",
    "\n",
    "Podemos definir una secuencia de processors general a ejecutar en el caso de que alguno de los processors del pipeline falle.\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [ ... ],\n",
    "  \"on_failure\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"description\": \"Index document to 'failed-<index>'\",\n",
    "        \"field\": \"_index\",\n",
    "        \"value\": \"failed-{{{ _index }}}\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Podemos encontrar información adicional del fallo en los metadatos del documento que son accesibles en el bloque \"on_failure\": on_failure_message, on_failure_processor_type, on_failure_processor_tag y on_failure_pipeline\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/my-pipeline\n",
    "{\n",
    "  \"processors\": [ ... ],\n",
    "  \"on_failure\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"description\": \"Record error information\",\n",
    "        \"field\": \"error_information\",\n",
    "        \"value\": \"Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casos de uso más comunes\n",
    "\n",
    "Vamos a ver algunos ejemplos a través de casos de uso muy comunes que nos podemos encontrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parsear los valores de los campos y extraer valores útiles para nuevos campos.\n",
    "\n",
    "Processors utilizados:\n",
    "* dissect: https://www.elastic.co/guide/en/elasticsearch/reference/current/dissect-processor.html#dissect-processor\n",
    "* lowercase: https://www.elastic.co/guide/en/elasticsearch/reference/current/lowercase-processor.html#lowercase-processor\n",
    "* remove: https://www.elastic.co/guide/en/elasticsearch/reference/current/remove-processor.html#remove-processor\n",
    "\n",
    "Vamos a suponer que los documentos a insertar contienen para el campo \"message\" cadenas de texto con el siguiente formato:\n",
    "\n",
    "`\n",
    "\"10:12:05 HTTP Monitor production is in GREEN state\"\n",
    "\"10:12:05 HTTP Monitor production is in RED state\"\n",
    "`\n",
    "\n",
    "En este caso hacen referencia a líneas de un fichero de log.\n",
    "\n",
    "Con el processor dissect podemos parsear estas cadenas y extraer ciertos valores e insertarlos en nuevos campos en el documento de entrada. \n",
    "\n",
    "Por ejemplo, vamos a extraer la hora el nombre del monitor y el estado para insertarlo en los respectivos campos:\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-one\n",
    "{\n",
    "  \"description\": \"Parse and extract log useful fields\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"dissect\": {\n",
    "        \"field\": \"message\",\n",
    "        \"pattern\": \"%{time} HTTP Monitor %{monitor.name} is in %{monitor.state} state\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Probemos el pipeline utilizando la función sumulate:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-one/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"message\": \"10:12:05 HTTP Monitor production is in GREEN state\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Para completar el pipeline, vamos a convertir en minúsculas el nuevo campo \"monitor.state\" utilizando el processor lowercase y a eliminar el campo original \"message\" con el processor remove. Lo haremos modificando el pipeline anteriror:\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-one\n",
    "{\n",
    "  \"description\": \"Parse and extract log useful fields\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"dissect\": {\n",
    "        \"field\": \"message\",\n",
    "        \"pattern\": \"%{time} HTTP Monitor %{monitor.name} is in %{monitor.state} state\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"lowercase\": {\n",
    "        \"field\": \"monitor.state\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"remove\": {\n",
    "        \"field\": \"message\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobamos que el pipeline se ha modificado correctamente:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-one/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"message\": \"10:12:05 HTTP Monitor production is in GREEN state\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tagear un documento en función del valor de un campo de documento base\n",
    "\n",
    "En este caso vamos a utilizar sentencias condicionales para crear un nuevo campo y asignarle un valor en función del contenido de un campo del documento base.\n",
    "\n",
    "Processors utilizados:\n",
    "* set: https://www.elastic.co/guide/en/elasticsearch/reference/current/set-processor.html#set-processor\n",
    "\n",
    "\n",
    "Suponemos que tenemos los siguientes documentos base:\n",
    "\n",
    "`\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"subnet\": \"CTS-01\",\n",
    "  \"classification\": \"secret\"\n",
    "}\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"subnet\": \"ATT-01\",\n",
    "  \"classification\": \"unclassified\"\n",
    "}\n",
    "`\n",
    "\n",
    "El processor set utiliza un script para comprobar los valores de los campos \"classification\" y \"subnet\", si los campos cumplen la condición del script entonces se tagean con el vaor \"protected\" añadiendo dicho valor al campo \"tag\".\n",
    "\n",
    "El campo \"if\" utiliza painless para definir la condición.\n",
    "\n",
    "Nota: \"ctx\" hace referencia al documento base.\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-two\n",
    "{\n",
    "  \"description\": \"Tag a document based on field value\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"if\": \"ctx.classification=='secret' && ctx.subnet=='CTS-01'\",\n",
    "        \"field\": \"tag\",\n",
    "        \"value\": \"protected\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobemos que el pipeline funciona correctamente:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-two/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"subnet\": \"CTS-01\",\n",
    "        \"classification\": \"secret\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"subnet\": \"ATT-01\",\n",
    "        \"classification\": \"unclassified\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descartar eventos de log no deseados basándose en los valores de los campos de tal forma que no se ingesten en el índice.\n",
    "\n",
    "En este caso vamos a descartar documentos en función del contenido de uno o varios campos. Para ello vamos primero a crear un uevo campo \"tag\" con el valor \"drop\" si cumple la condición descrita por el processor script y después vamos a descartar los documentos cuyo campo \"tag\" contengan el valor \"drop\" utilizando el porcessor drop.\n",
    "\n",
    "Processors utilizados:\n",
    "* script: https://www.elastic.co/guide/en/elasticsearch/reference/current/drop-processor.html#drop-processor\n",
    "* drop: https://www.elastic.co/guide/en/elasticsearch/reference/current/drop-processor.html#drop-processor\n",
    "\n",
    "El formato de los ducumentos base es el siguiente:\n",
    "\n",
    "`\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"subnet\": \"CTS-01\",\n",
    "  \"event_code\": \"AS-32\"\n",
    "}\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"subnet\": \"ATT-01\",\n",
    "  \"event_code\": \"AS-29\"\n",
    "}\n",
    "`\n",
    "\n",
    "Creamos el pipeline que procese los documentos y descarte aquellos que tienen un event_code no permitido:\n",
    "El processor script permite utilizar varios lenguajes de scripting:\n",
    "* painless: Por defecto sino se indica el calpo \"lang\". Leguaje de scripting propio de Elasticsearch. https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html#modules-scripting-painless\n",
    "* expression: Lenguaje de scripting de Lucene. https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-expression.html#modules-scripting-expression\n",
    "* mustache: Pensado para templates. https://mustache.github.io/\n",
    "* Java: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html#modules-scripting-painless\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-three\n",
    "{\n",
    "  \"description\": \"Dop document based on field value\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"script\": {\n",
    "        \"lang\": \"painless\",\n",
    "        \"source\": \"\"\"\n",
    "          def disallowedCodes = [\"AS-29\",\"BA-23\"];\n",
    "          if (disallowedCodes.contains(ctx.event_code)) {\n",
    "            ctx.tag = \"drop\";\n",
    "          }\n",
    "          \"\"\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "      \"drop\": {\n",
    "        \"if\": \"ctx.tag == 'drop'\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobemos el correcto funcionamiento del pipeline que acabamos de definir:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-three/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"subnet\": \"CTS-01\",\n",
    "        \"event_code\": \"AS-32\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"subnet\": \"ATT-01\",\n",
    "        \"event_code\": \"AS-29\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Enrutar e indexar documentos en el ídice correcto de Elasticsearch basándose en el valor de los campos del documento.\n",
    "\n",
    "Para indicar en que índice se insertará un documento, vamos a sobreescribir el campo \"_index\" que le indica a elasticsearch que índice es el que debe utilizar para indexar un documento.\n",
    "\n",
    "Processors utilizados:\n",
    "* set: https://www.elastic.co/guide/en/elasticsearch/reference/current/set-processor.html#set-processor\n",
    "\n",
    "Para ello utilizaremos el processor set y asignaremos al campo \"_index\" el resultado de concatenar el valor del campo \"application\" del documento base con el valor del campo \"environment\" del documento base. De esta forma conseguiremos tener separados en diferentes índices los documentos de cada aplicación y por entorno.\n",
    "\n",
    "Los documentos base tienen el siguiente formato:\n",
    "\n",
    "`\n",
    "{\n",
    "  \"environment\": \"production\",\n",
    "  \"application\": \"apache\"\n",
    "}\n",
    "{\n",
    "  \"environment\": \"dev\",\n",
    "  \"application\": \"apache\"\n",
    "}\n",
    "`\n",
    "\n",
    "\n",
    "Creamos el pipeline:\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-four\n",
    "{\n",
    "  \"description\": \"Route document into correct index based on field value\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"field\": \"_index\",\n",
    "        \"value\": \"{{application}}-{{environment}}\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobemos el correcto funcionamiento del pipeline:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-four/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"application\": \"apache\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"environment\": \"dev\",\n",
    "        \"application\": \"apache\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Enmascarar información sensible almacenada en los valores de los campos del documento.\n",
    "\n",
    "Otro caso de uso muy util es poder enmascarar información sensible como en este caso los números de tarjetas de crédito. \n",
    "\n",
    "Processors utilizados:\n",
    "* gsub: https://www.elastic.co/guide/en/elasticsearch/reference/current/gsub-processor.html#gsub-processor\n",
    "\n",
    "\n",
    "Vamos a suponer el siguiente formato de documento base:\n",
    "\n",
    "`\n",
    "{\n",
    "  \"message\": \"Customer A1121 paid with 5555555555554444\"\n",
    "}\n",
    "{\n",
    "  \"message\": \"Customer A1122 paid with 378282246310005\"\n",
    "}\n",
    "`\n",
    "\n",
    "Para ello vamos a definir una expresión regular que sea capaz de extraer el número de una tarjeta de credito y lo utilizaremos con el processor gsub que permite modificar una cadena aplicando expresiones regulares.\n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-five\n",
    "{\n",
    "  \"description\": \"Strip sensitive information from the fields in documents\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"gsub\": {\n",
    "        \"field\": \"message\",\n",
    "        \"pattern\": \"\\\\b(?:3[47]\\\\d|(?:4\\\\d|5[1-5]|65)\\\\d{2}|6011)\\\\d{12}\\\\b\",\n",
    "        \"replacement\": \"xxxx-xxxx-xxxx-xxxx\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Probamos el funcionamiento del pipeline:\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-five/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"message\": \"Customer A1121 paid with 5555555555554444\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"message\": \"Customer A1122 paid with 378282246310005\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Enriquecer la el campo que contiene una dirección IP pública con información geográfica.\n",
    "\n",
    "También podemos utilizar el processor geoip para buscar las coordenadas geográficas de una IP y enriquecer el documento base con esta información.\n",
    "\n",
    "Processors utilizados:\n",
    "* geoip: https://www.elastic.co/guide/en/elasticsearch/reference/current/geoip-processor.html#geoip-processor\n",
    "* rename: https://www.elastic.co/guide/en/elasticsearch/reference/current/rename-processor.html#rename-processor\n",
    "\n",
    "El formato del documento base que utilizaremos es:\n",
    "`\n",
    "{\n",
    "  \"source_ip\": \"194.121.12.154\"\n",
    "}\n",
    "`\n",
    "\n",
    "Definimos el pipeline para ello primero extraemos las coordenadas geográficas a partir del campo \"soruce_ip\" del documento base y dejamos los valores obtenidos en el campo \"source.geo\". Después renombramos el campo source_ip por \"source.ip\" para embeberlo dendro del documento source junto con la información geográfica. \n",
    "\n",
    "`\n",
    "PUT _ingest/pipeline/processors-example-six\n",
    "{\n",
    "  \"description\": \"Enrich the public IP address fields with geo-location information\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"geoip\": {\n",
    "        \"field\": \"source_ip\",\n",
    "        \"target_field\": \"source.geo\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"rename\": {\n",
    "        \"field\": \"source_ip\",\n",
    "        \"target_field\": \"source.ip\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n",
    "\n",
    "Comprobamos su funcionamiento. Puedes probar con tu IP publica.\n",
    "\n",
    "`\n",
    "POST _ingest/pipeline/processors-example-six/_simulate\n",
    "{\n",
    "  \"docs\": [\n",
    "    {\n",
    "      \"_source\": {\n",
    "        \"source_ip\": \"194.121.12.154\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Ahora es tu turno para practicar. Para ello vamos a ingestar un data set que contiene todo el catálogo disponible en Netflix a fecha de octubre de 2021 junto más información como el cating, directores, ratings, duración, etc..\n",
    "\n",
    "En este enlace puedes encontrar más información sobre el data set: https://www.kaggle.com/datasets/shivamb/netflix-shows?resource=download\n",
    "\n",
    "El fichero ya descargado lo puedes encontrar dentro de tu workspace en la siguiente ruta: work/data/elasticsearch/netflix/netflix_titles.csv\n",
    "\n",
    "El fichero está en formato csv donde cada línea es una entrada del catálogo. A continuación puedes encontrar tres ejemplos:\n",
    "\n",
    "show_id,type,title,director,cast,country,date_added,release_year,rating,duration,listed_in,description\n",
    "\n",
    "s1,Movie,Dick Johnson Is Dead,Kirsten Johnson,,United States,\"September 25, 2021\",2020,PG-13,90 min,Documentaries,\"As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.\"\n",
    "\n",
    "s2,TV Show,Blood & Water,,\"Ama Qamata, Khosi Ngema, Gail Mabalane, Thabang Molaba, Dillon Windvogel, Natasha Thahane, Arno Greeff, Xolile Tshabalala, Getmore Sithole, Cindy Mahlangu, Ryle De Morny, Greteli Fincham, Sello Maake Ka-Ncube, Odwa Gwanya, Mekaila Mathys, Sandi Schultz, Duane Williams, Shamilla Miller, Patrick Mofokeng\",South Africa,\"September 24, 2021\",2021,TV-MA,2 Seasons,\"International TV Shows, TV Dramas, TV Mysteries\",\"After crossing paths at a party, a Cape Town teen sets out to prove whether a private-school swimming star is her sister who was abducted at birth.\"\n",
    "\n",
    "s3,TV Show,Ganglands,Julien Leclercq,\"Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabiha Akkari, Sofia Lesaffre, Salim Kechiouche, Noureddine Farihi, Geert Van Rampelberg, Bakary Diombera\",,\"September 24, 2021\",2021,TV-MA,1 Season,\"Crime TV Shows, International TV Shows, TV Action & Adventure\",\"To protect his family from a powerful drug lord, skilled thief Mehdi and his expert team of robbers are pulled into a violent and deadly turf war.\"\n",
    "\n",
    "Para ingestar esta información vamos a crear un pipeline de ingesta para procesar e indexar correctamente los datos. \n",
    "\n",
    "### 1. Diseña el índice para lamacenar los datos\n",
    "\n",
    "Como primer paso, diseña el índice en el que almacenar los datos ingestados, pero no lo realices la llamada para crearlo. El índice tiene que cumplir las siguientes características:\n",
    "\n",
    "* **Nombre:** netflix_titles\n",
    "* **Campos:**\n",
    "    * cast, categories, country, director, duration, rating, show_id y type de tipo keyword\n",
    "    * description y title de tipo text\n",
    "    * date_added tipo date\n",
    "    * release_year de tipo long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "PUT netflix_titles\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"cast\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"categories\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"country\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"date_added\": {\n",
    "        \"type\": \"date\"\n",
    "      },\n",
    "      \"description\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"director\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"duration\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"rating\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"release_year\": {\n",
    "        \"type\": \"long\"\n",
    "      },\n",
    "      \"show_id\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"type\": {\n",
    "        \"type\": \"keyword\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Diseña un pipeline de ingesta que realice las siguientes operaciones:\n",
    "\n",
    "\n",
    "\n",
    "* **Nombre del pipeline**: \"netflix-titles-pipeline\"\n",
    "* **Processors**:\n",
    "    1. Utiliza el processor csv para procesar cada línea del fichero (https://www.elastic.co/guide/en/elasticsearch/reference/current/csv-processor.html).\n",
    "    2. Utiliza el processor split para separar los diferentes miembros del campo cast para crear un array de strings. El separador que tienes que usar es el siguiente: \",\\\\s*\". (https://www.elastic.co/guide/en/elasticsearch/reference/current/split-processor.html).\n",
    "    3. Utiliza el processor split igual que antes, pero para el campo listed_in.\n",
    "    4. Utiliza el processor remane para renombrar el campo listed_in con el nombre categories. (https://www.elastic.co/guide/en/elasticsearch/reference/current/rename-processor.html)\n",
    "    5. Utiliza el processor trim para eliminar los espacios en blanco del campo date_added para poderlo parsear adecuadamente (https://www.elastic.co/guide/en/elasticsearch/reference/current/trim-processor.html).\n",
    "    6. Usa el processor date para parsear la fecha del campo date_added y convertirlo en un campo de tipo date (https://www.elastic.co/guide/en/elasticsearch/reference/current/date-processor.html).\n",
    "    7. Utiliza el processor convert para convertir el campo release_year en un capo de tipo integer (https://www.elastic.co/guide/en/elasticsearch/reference/current/convert-processor.html).\n",
    "    8. Utiliza el processor remove para eliminar el campo message con el documento original (https://www.elastic.co/guide/en/elasticsearch/reference/current/remove-processor.html).\n",
    "* Configura todos los processors para que ignore los registros que no contengan los campos implicados.\n",
    "* Gestiona los fallos en el pipeline de forma global para que envíe los documentos que han dado error y no se han podido parsear a un índice llamado \"dead-letter-(nombre del índice en el que se intentaba insertar el documento).\n",
    "\n",
    "Continúa al partir del siguiente procesor:\n",
    "\n",
    "`\n",
    "      {\n",
    "        \"csv\": {\n",
    "          \"description\": \"Parse the incoming message\",\n",
    "          \"field\": \"message\",\n",
    "          \"target_fields\": [\n",
    "            \"show_id\",\n",
    "            \"type\",\n",
    "            \"title\",\n",
    "            \"director\",\n",
    "            \"cast\",\n",
    "            \"country\",\n",
    "            \"date_added\",\n",
    "            \"release_year\",\n",
    "            \"rating\",\n",
    "            \"duration\",\n",
    "            \"listed_in\",\n",
    "            \"description\"\n",
    "          ],\n",
    "          \"trim\": true,\n",
    "          \"tag\": \"csv-parse-message\"\n",
    "        }\n",
    "      }\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "PUT _ingest/pipeline/netflix-titles-pipeline\n",
    "{\n",
    "  \"processors\": [\n",
    "      {\n",
    "        \"csv\": {\n",
    "          \"description\": \"Parse the incoming message\",\n",
    "          \"field\": \"message\",\n",
    "          \"target_fields\": [\n",
    "            \"show_id\",\n",
    "            \"type\",\n",
    "            \"title\",\n",
    "            \"director\",\n",
    "            \"cast\",\n",
    "            \"country\",\n",
    "            \"date_added\",\n",
    "            \"release_year\",\n",
    "            \"rating\",\n",
    "            \"duration\",\n",
    "            \"listed_in\",\n",
    "            \"description\"\n",
    "          ],\n",
    "          \"trim\": true,\n",
    "          \"tag\": \"csv-parse-message\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"split\": {\n",
    "          \"description\": \"Split the cast property into cast members\",\n",
    "          \"field\": \"cast\",\n",
    "          \"separator\": \",\\\\s*\",\n",
    "          \"ignore_missing\": true,\n",
    "          \"tag\": \"split-cast\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"split\": {\n",
    "          \"description\": \"Split the listed_in property into categories\",\n",
    "          \"field\": \"listed_in\",\n",
    "          \"separator\": \",\\\\s*\",\n",
    "          \"ignore_missing\": true,\n",
    "          \"tag\": \"split-listed_in\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"rename\": {\n",
    "          \"description\": \"Rename the listed_in property in categories\",\n",
    "          \"field\": \"listed_in\",\n",
    "          \"target_field\": \"categories\", \n",
    "          \"ignore_missing\": true,\n",
    "          \"tag\": \"rename-listed_in\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"trim\": {\n",
    "          \"description\": \"Trim date_added field\",\n",
    "          \"field\": \"date_added\",\n",
    "          \"ignore_missing\": true,\n",
    "          \"tag\": \"trim-date_added\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"date\": {\n",
    "          \"description\": \"Convert date_added field to a date field\",\n",
    "          \"field\": \"date_added\",\n",
    "          \"formats\": [ \"MMMM d, yyyy\"],\n",
    "          \"target_field\": \"date_added\",\n",
    "          \"tag\": \"date-date_added\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"convert\": {\n",
    "          \"description\": \"Convert release_year to a number\",\n",
    "          \"field\": \"release_year\",\n",
    "          \"type\": \"integer\",\n",
    "          \"tag\": \"convert-release_year\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"remove\": {\n",
    "          \"description\": \"Finally remove the message field\",\n",
    "          \"field\": \"message\",\n",
    "          \"tag\": \"remove-message\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"on_failure\": [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"description\": \"Index document to 'failed-<index>'\",\n",
    "        \"field\": \"_index\",\n",
    "        \"value\": \"dead-letter-{{{ _index }}}\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prueba el funcionamiento de tu pipeline ingestando los datos del csv:\n",
    "\n",
    "Vamos a parsear el fichero e insertarlo en elasticsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting certifi==2021.5.30\n",
      "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "     |████████████████████████████████| 145 kB 995 kB/s            \n",
      "\u001b[?25hInstalling collected packages: certifi\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2018.8.24\n",
      "\u001b[31mERROR: Cannot uninstall 'certifi'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "Collecting elasticsearch==7.13.1\n",
      "  Downloading elasticsearch-7.13.1-py2.py3-none-any.whl (354 kB)\n",
      "     |████████████████████████████████| 354 kB 1.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from elasticsearch==7.13.1) (1.23)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.6/site-packages (from elasticsearch==7.13.1) (2018.8.24)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.13.1\n",
      "Collecting urllib3==1.26.6\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "     |████████████████████████████████| 138 kB 751 kB/s            \n",
      "\u001b[?25hInstalling collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.23\n",
      "    Uninstalling urllib3-1.23:\n",
      "      Successfully uninstalled urllib3-1.23\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.19.1 requires urllib3<1.24,>=1.21.1, but you have urllib3 1.26.6 which is incompatible.\u001b[0m\n",
      "Successfully installed urllib3-1.26.6\n"
     ]
    }
   ],
   "source": [
    "!pip install certifi==2021.5.30\n",
    "!pip install elasticsearch==7.13.1\n",
    "!pip install urllib3==1.26.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import streaming_bulk\n",
    "\n",
    "def generate_bulk_actions(file, index, pipeline, skip_first_line, max_docs):\n",
    "    with open(file, encoding = 'UTF-8') as f:\n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            if skip_first_line and idx ==0:\n",
    "                continue\n",
    "            if max_docs != -1:\n",
    "                stop_idx = max_docs + 1 if skip_first_line else max_docs\n",
    "                if idx == stop_idx:\n",
    "                    break\n",
    "            action = {\n",
    "                '_index': index,\n",
    "                '_source': {\n",
    "                    'message': line.strip()\n",
    "                }\n",
    "            }\n",
    "            if pipeline:\n",
    "                action['pipeline'] = pipeline\n",
    "            yield action\n",
    "\n",
    "def run(file, index, pipeline):\n",
    "    es_host = 'http://elasticsearch:9200'\n",
    "\n",
    "    es = Elasticsearch([es_host])\n",
    "    print('-- elasticsearch host set to :', es_host)\n",
    "\n",
    "    successes = 0\n",
    "    for ok, _ in streaming_bulk(client=es, actions= generate_bulk_actions(file, index, pipeline, True, -1)):\n",
    "        successes += ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- elasticsearch host set to : http://elasticsearch:9200\n"
     ]
    }
   ],
   "source": [
    "run('../data/elasticsearch/netflix/netflix_titles.csv', 'netflix_titles', 'netflix-titles-pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Comprueba que se han insertado correctamente los documentos.\n",
    "2. ¿Cuántos documentos no se han parseado correctamente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
